{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06dc5d9f-9c1f-49da-9502-acccac85a145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c901f8a3-046a-4aed-901e-19fa7139134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MovieRecSys\").getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "648c84d7-5dd4-4596-b632-7ff6b15f9e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|    31|     47|   5.0|\n",
      "|    31|     50|   4.0|\n",
      "|    31|    296|   4.5|\n",
      "|    31|    318|   5.0|\n",
      "|    31|    608|   4.5|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "Rows after limiting: 8683\n",
      "Unique users: 50\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Read ratings.csv\n",
    "df = spark.read.csv(\"ratings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select first 1000 unique users\n",
    "unique_users = df.select(\"userId\").distinct().limit(50)\n",
    "\n",
    "# Keep only rows from these users\n",
    "df_limited = df.join(unique_users, on=\"userId\", how=\"inner\")\n",
    "\n",
    "df_limited.show(5)\n",
    "print(\"Rows after limiting:\", df_limited.count())\n",
    "print(\"Unique users:\", df_limited.select(\"userId\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a9b237f-662e-4628-8d8d-20c8f72a41b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+\n",
      "|user|movie|rating_scaled|\n",
      "+----+-----+-------------+\n",
      "|31.0| 15.0|          1.0|\n",
      "|31.0|  7.0|          0.8|\n",
      "|31.0|  3.0|          0.9|\n",
      "|31.0|  4.0|          1.0|\n",
      "|31.0| 12.0|          0.9|\n",
      "+----+-----+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, MinMaxScaler, VectorAssembler\n",
    "from pyspark.sql.functions import col, round as spark_round\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Encode users\n",
    "user_indexer = StringIndexer(inputCol=\"userId\", outputCol=\"user\")\n",
    "df_small = user_indexer.fit(df_limited).transform(df_limited)\n",
    "\n",
    "# Encode movies\n",
    "movie_indexer = StringIndexer(inputCol=\"movieId\", outputCol=\"movie\")\n",
    "df_small = movie_indexer.fit(df_small).transform(df_small)\n",
    "\n",
    "# Min-Max on Ratings\n",
    "assembler = VectorAssembler(inputCols=[\"rating\"], outputCol=\"rating_vec\")\n",
    "df_vec = assembler.transform(df_small)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"rating_vec\", outputCol=\"rating_scaled\")\n",
    "scaler_model = scaler.fit(df_vec)\n",
    "scaled_df = scaler_model.transform(df_vec)\n",
    "\n",
    "# Flatten vector -> float, then round to 1 decimal\n",
    "scaled_df = scaled_df.withColumn(\n",
    "    \"rating_scaled\",\n",
    "    spark_round(vector_to_array(col(\"rating_scaled\"))[0], 1)\n",
    ")\n",
    "\n",
    "# Keep only necessary columns\n",
    "df_final = scaled_df.select(\"user\", \"movie\", \"rating_scaled\")\n",
    "df_final.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0577f51-33aa-4c3b-b8a4-b4856eec7c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 7010\n",
      "Train Label: 7010\n",
      "Test count: 1673\n",
      "Test Label: 1673\n"
     ]
    }
   ],
   "source": [
    "# 80% train, 20% test\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "#Converting PySpark DF to Pandas to Tensors\n",
    "train_pd = train_df.toPandas()\n",
    "test_pd = test_df.toPandas()\n",
    "\n",
    "X_train = torch.tensor(train_pd[[\"user\", \"movie\"]].values)\n",
    "y_train = torch.tensor(train_pd[\"rating_scaled\"].values)\n",
    "\n",
    "X_test = torch.tensor(test_pd[[\"user\", \"movie\"]].values)\n",
    "y_test = torch.tensor(test_pd[\"rating_scaled\"].values)\n",
    "\n",
    "print(\"Train count:\", X_train.shape[0])\n",
    "print(\"Train Label:\",y_train.shape[0])\n",
    "print(\"Test count:\", X_test.shape[0])\n",
    "print(\"Test Label:\",y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbe525d8-319a-45f5-957e-bcc953b8e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "#print(train_dataset[2200])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95de7640-7647-4ae2-b1a3-fc3b8419eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CollabFiltering(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, emb_dim, hidden, dropout_p):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, hidden),  # user+movie embeddings concatenated\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden, 1),\n",
    "            nn.Sigmoid()  # predict rating\n",
    "        )\n",
    "\n",
    "    def forward(self,user,movie):\n",
    "        u = self.user_emb(user)\n",
    "        m = self.movie_emb(movie)\n",
    "        \n",
    "        # Concatenate embeddings (instead of dot product)\n",
    "        x = torch.cat([u, m], dim=1)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        return self.mlp(x).squeeze()\n",
    "\n",
    "n_users = df_final.select(\"user\").distinct().count()\n",
    "n_movies = df_final.select(\"movie\").distinct().count()\n",
    "\n",
    "model = CollabFiltering(n_users, n_movies, emb_dim=16, hidden=32, dropout_p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa15af3b-4e8a-4653-ba73-7be231945a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of CollabFiltering(\n",
      "  (user_emb): Embedding(50, 16)\n",
      "  (movie_emb): Embedding(3524, 16)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a02e26b8-c260-4f5a-8150-fd7ab72229d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss() # regression on ratings\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da2069b2-cd61-4f80-a1b3-e7504fbc5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(parameter, m_state, v_state, t, lr=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    # Loop over Tensors in model.parameters()\n",
    "    for p in parameter:\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "\n",
    "        # First Time seeing this parameter\n",
    "        if p not in m_state:\n",
    "            m_state[p] = torch.zeros_like(p)\n",
    "            v_state[p] = torch.zeros_like(p)\n",
    "\n",
    "        g = p.grad\n",
    "        # First moment\n",
    "        m_state[p] = beta1 * m_state[p] + (1 - beta1) * g\n",
    "        # Second moment\n",
    "        v_state[p] = beta2 * v_state[p] + (1 - beta2) * (g * g)\n",
    "\n",
    "        # Bias correction\n",
    "        m_corrected = m_state[p] / (1 - beta1 ** t)\n",
    "        v_corrected = v_state[p] / (1 - beta2 ** t)\n",
    "\n",
    "        # Parameter update (in-place)\n",
    "        p.data -= lr * m_corrected / (torch.sqrt(v_corrected) + epsilon)\n",
    "\n",
    "    return m_state, v_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "271f4566-0153-4793-8b0b-2ffa3f81a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Train Loss: 0.0102, Train RMSE: 0.1012\n",
      "Test  Loss: 0.0695, Test  RMSE: 0.2516\n",
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 0.0099, Train RMSE: 0.0993\n",
      "Test  Loss: 0.0708, Test  RMSE: 0.2536\n",
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 0.0099, Train RMSE: 0.0993\n",
      "Test  Loss: 0.0711, Test  RMSE: 0.2554\n",
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 0.0102, Train RMSE: 0.1006\n",
      "Test  Loss: 0.0689, Test  RMSE: 0.2522\n",
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 0.0099, Train RMSE: 0.0994\n",
      "Test  Loss: 0.0697, Test  RMSE: 0.2532\n",
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0100, Train RMSE: 0.1001\n",
      "Test  Loss: 0.0688, Test  RMSE: 0.2518\n",
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0098, Train RMSE: 0.0989\n",
      "Test  Loss: 0.0700, Test  RMSE: 0.2533\n",
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0098, Train RMSE: 0.0992\n",
      "Test  Loss: 0.0699, Test  RMSE: 0.2528\n",
      "\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0097, Train RMSE: 0.0982\n",
      "Test  Loss: 0.0711, Test  RMSE: 0.2553\n",
      "\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0097, Train RMSE: 0.0985\n",
      "Test  Loss: 0.0705, Test  RMSE: 0.2549\n",
      "\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0092, Train RMSE: 0.0959\n",
      "Test  Loss: 0.0711, Test  RMSE: 0.2553\n",
      "\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0094, Train RMSE: 0.0970\n",
      "Test  Loss: 0.0688, Test  RMSE: 0.2510\n",
      "\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0093, Train RMSE: 0.0967\n",
      "Test  Loss: 0.0714, Test  RMSE: 0.2563\n",
      "\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0093, Train RMSE: 0.0963\n",
      "Test  Loss: 0.0717, Test  RMSE: 0.2561\n",
      "\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0098, Train RMSE: 0.0989\n",
      "Test  Loss: 0.0672, Test  RMSE: 0.2488\n",
      "\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0092, Train RMSE: 0.0959\n",
      "Test  Loss: 0.0706, Test  RMSE: 0.2537\n",
      "\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0090, Train RMSE: 0.0948\n",
      "Test  Loss: 0.0699, Test  RMSE: 0.2530\n",
      "\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0092, Train RMSE: 0.0960\n",
      "Test  Loss: 0.0698, Test  RMSE: 0.2524\n",
      "\n",
      "Epoch 19/20\n",
      "Train Loss: 0.0091, Train RMSE: 0.0955\n",
      "Test  Loss: 0.0706, Test  RMSE: 0.2538\n",
      "\n",
      "Epoch 20/20\n",
      "Train Loss: 0.0092, Train RMSE: 0.0958\n",
      "Test  Loss: 0.0709, Test  RMSE: 0.2548\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "t = 0\n",
    "m_state, v_state = {}, {}\n",
    "train_losses, test_losses = [], []\n",
    "train_rmses, test_rmses = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # ---------- Training ----------\n",
    "    model.train()\n",
    "    total_loss, total_sq_error, total_samples = 0, 0, 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        user_batch = X_batch[:, 0].long()\n",
    "        movie_batch = X_batch[:, 1].long()\n",
    "        \n",
    "        preds = model(user_batch, movie_batch).squeeze()\n",
    "        loss = loss_fn(preds, y_batch.float())\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        # Adam\n",
    "        t += 1\n",
    "        m_state, v_state = adam(model.parameters(), m_state, v_state,t)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_sq_error += torch.sum((preds - y_batch) ** 2).item()\n",
    "        total_samples += len(y_batch)\n",
    "    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_rmse = (total_sq_error / total_samples) ** 0.5\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmses.append(train_rmse)\n",
    "    \n",
    "    # ---------- Testing ----------\n",
    "    model.eval()\n",
    "    total_loss, total_sq_error, total_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            user_batch = X_batch[:, 0].long()\n",
    "            movie_batch = X_batch[:, 1].long()\n",
    "            \n",
    "            preds = model(user_batch, movie_batch).squeeze()\n",
    "            loss = loss_fn(preds, y_batch.float())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_sq_error += torch.sum((preds - y_batch) ** 2).item()\n",
    "            total_samples += len(y_batch)\n",
    "    \n",
    "    test_loss = total_loss / len(test_loader)\n",
    "    test_rmse = (total_sq_error / total_samples) ** 0.5\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmses.append(test_rmse)\n",
    "    \n",
    "    # ---------- Summary ----------\n",
    "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Test  Loss: {test_loss:.4f}, Test  RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a479-b6c2-4337-9d37-f277a5a6bf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GDP]",
   "language": "python",
   "name": "conda-env-GDP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

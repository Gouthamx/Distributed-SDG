{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06dc5d9f-9c1f-49da-9502-acccac85a145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c901f8a3-046a-4aed-901e-19fa7139134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MovieRecSys\").getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "648c84d7-5dd4-4596-b632-7ff6b15f9e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|    31|     47|   5.0|\n",
      "|    31|     50|   4.0|\n",
      "|    31|    296|   4.5|\n",
      "|    31|    318|   5.0|\n",
      "|    31|    608|   4.5|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "Rows after limiting: 8683\n",
      "Unique users: 50\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Read ratings.csv\n",
    "df = spark.read.csv(\"ratings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select first 1000 unique users\n",
    "unique_users = df.select(\"userId\").distinct().limit(50)\n",
    "\n",
    "# Keep only rows from these users\n",
    "df_limited = df.join(unique_users, on=\"userId\", how=\"inner\")\n",
    "\n",
    "df_limited.show(5)\n",
    "print(\"Rows after limiting:\", df_limited.count())\n",
    "print(\"Unique users:\", df_limited.select(\"userId\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a9b237f-662e-4628-8d8d-20c8f72a41b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+\n",
      "|user|movie|rating_scaled|\n",
      "+----+-----+-------------+\n",
      "|31.0| 15.0|          1.0|\n",
      "|31.0|  7.0|          0.8|\n",
      "|31.0|  3.0|          0.9|\n",
      "|31.0|  4.0|          1.0|\n",
      "|31.0| 12.0|          0.9|\n",
      "+----+-----+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, MinMaxScaler, VectorAssembler\n",
    "from pyspark.sql.functions import col, round as spark_round\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Encode users\n",
    "user_indexer = StringIndexer(inputCol=\"userId\", outputCol=\"user\")\n",
    "df_small = user_indexer.fit(df_limited).transform(df_limited)\n",
    "\n",
    "# Encode movies\n",
    "movie_indexer = StringIndexer(inputCol=\"movieId\", outputCol=\"movie\")\n",
    "df_small = movie_indexer.fit(df_small).transform(df_small)\n",
    "\n",
    "# Min-Max on Ratings\n",
    "assembler = VectorAssembler(inputCols=[\"rating\"], outputCol=\"rating_vec\")\n",
    "df_vec = assembler.transform(df_small)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"rating_vec\", outputCol=\"rating_scaled\")\n",
    "scaler_model = scaler.fit(df_vec)\n",
    "scaled_df = scaler_model.transform(df_vec)\n",
    "\n",
    "# Flatten vector -> float, then round to 1 decimal\n",
    "scaled_df = scaled_df.withColumn(\n",
    "    \"rating_scaled\",\n",
    "    spark_round(vector_to_array(col(\"rating_scaled\"))[0], 1)\n",
    ")\n",
    "\n",
    "# Keep only necessary columns\n",
    "df_final = scaled_df.select(\"user\", \"movie\", \"rating_scaled\")\n",
    "df_final.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0577f51-33aa-4c3b-b8a4-b4856eec7c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 7010\n",
      "Train Label: 7010\n",
      "Test count: 1673\n",
      "Test Label: 1673\n"
     ]
    }
   ],
   "source": [
    "# 80% train, 20% test\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "#Converting PySpark DF to Pandas to Tensors\n",
    "train_pd = train_df.toPandas()\n",
    "test_pd = test_df.toPandas()\n",
    "\n",
    "X_train = torch.tensor(train_pd[[\"user\", \"movie\"]].values)\n",
    "y_train = torch.tensor(train_pd[\"rating_scaled\"].values)\n",
    "\n",
    "X_test = torch.tensor(test_pd[[\"user\", \"movie\"]].values)\n",
    "y_test = torch.tensor(test_pd[\"rating_scaled\"].values)\n",
    "\n",
    "print(\"Train count:\", X_train.shape[0])\n",
    "print(\"Train Label:\",y_train.shape[0])\n",
    "print(\"Test count:\", X_test.shape[0])\n",
    "print(\"Test Label:\",y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbe525d8-319a-45f5-957e-bcc953b8e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "#print(train_dataset[2200])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95de7640-7647-4ae2-b1a3-fc3b8419eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CollabFiltering(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, emb_dim=8,hidden=8,dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_users * 2, hidden),  # user+movie embeddings concatenated\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden, 1),\n",
    "            nn.Sigmoid()  # predict rating\n",
    "        )\n",
    "\n",
    "    def forward(self,user,movie):\n",
    "        u = self.user_emb(user)\n",
    "        m = self.movie_emb(movie)\n",
    "        \n",
    "        # Concatenate embeddings (instead of dot product)\n",
    "        x = torch.cat([u, m], dim=1)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        return self.mlp(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73c28a0c-067d-4e70-a39a-c85e534ac0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df_final.select(\"user\").distinct().count()\n",
    "n_movies = df_final.select(\"movie\").distinct().count()\n",
    "\n",
    "model = CollabFiltering(n_users, n_movies, emb_dim=50, hidden=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a02e26b8-c260-4f5a-8150-fd7ab72229d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss()  # regression on ratings\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "271f4566-0153-4793-8b0b-2ffa3f81a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Train Loss: 0.0020, Train RMSE: 0.0640\n",
      "Test  Loss: 0.0284, Test  RMSE: 0.2264\n",
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 0.0028, Train RMSE: 0.0742\n",
      "Test  Loss: 0.0284, Test  RMSE: 0.2264\n",
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 0.0024, Train RMSE: 0.0689\n",
      "Test  Loss: 0.0284, Test  RMSE: 0.2264\n",
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 0.0023, Train RMSE: 0.0677\n",
      "Test  Loss: 0.0291, Test  RMSE: 0.2284\n",
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 0.0021, Train RMSE: 0.0655\n",
      "Test  Loss: 0.0291, Test  RMSE: 0.2287\n",
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0021, Train RMSE: 0.0654\n",
      "Test  Loss: 0.0292, Test  RMSE: 0.2289\n",
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0021, Train RMSE: 0.0646\n",
      "Test  Loss: 0.0291, Test  RMSE: 0.2284\n",
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0021, Train RMSE: 0.0642\n",
      "Test  Loss: 0.0293, Test  RMSE: 0.2296\n",
      "\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0020, Train RMSE: 0.0631\n",
      "Test  Loss: 0.0292, Test  RMSE: 0.2286\n",
      "\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0020, Train RMSE: 0.0632\n",
      "Test  Loss: 0.0291, Test  RMSE: 0.2283\n",
      "\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0019, Train RMSE: 0.0622\n",
      "Test  Loss: 0.0291, Test  RMSE: 0.2280\n",
      "\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0020, Train RMSE: 0.0638\n",
      "Test  Loss: 0.0293, Test  RMSE: 0.2290\n",
      "\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0019, Train RMSE: 0.0623\n",
      "Test  Loss: 0.0292, Test  RMSE: 0.2290\n",
      "\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0020, Train RMSE: 0.0626\n",
      "Test  Loss: 0.0290, Test  RMSE: 0.2288\n",
      "\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0019, Train RMSE: 0.0619\n",
      "Test  Loss: 0.0289, Test  RMSE: 0.2282\n",
      "\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0019, Train RMSE: 0.0609\n",
      "Test  Loss: 0.0292, Test  RMSE: 0.2293\n",
      "\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0018, Train RMSE: 0.0605\n",
      "Test  Loss: 0.0290, Test  RMSE: 0.2287\n",
      "\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0018, Train RMSE: 0.0600\n",
      "Test  Loss: 0.0292, Test  RMSE: 0.2288\n",
      "\n",
      "Epoch 19/20\n",
      "Train Loss: 0.0018, Train RMSE: 0.0604\n",
      "Test  Loss: 0.0293, Test  RMSE: 0.2298\n",
      "\n",
      "Epoch 20/20\n",
      "Train Loss: 0.0017, Train RMSE: 0.0591\n",
      "Test  Loss: 0.0295, Test  RMSE: 0.2292\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "train_losses, test_losses = [], []\n",
    "train_rmses, test_rmses = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # ---------- Training ----------\n",
    "    model.train()\n",
    "    total_loss, total_sq_error, total_samples = 0, 0, 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        user_batch = X_batch[:, 0].long()\n",
    "        movie_batch = X_batch[:, 1].long()\n",
    "        \n",
    "        preds = model(user_batch, movie_batch).squeeze()\n",
    "        loss = loss_fn(preds, y_batch.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_sq_error += torch.sum((preds - y_batch) ** 2).item()\n",
    "        total_samples += len(y_batch)\n",
    "    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_rmse = (total_sq_error / total_samples) ** 0.5\n",
    "    train_losses.append(train_loss)\n",
    "    train_rmses.append(train_rmse)\n",
    "    \n",
    "    # ---------- Testing ----------\n",
    "    model.eval()\n",
    "    total_loss, total_sq_error, total_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            user_batch = X_batch[:, 0].long()\n",
    "            movie_batch = X_batch[:, 1].long()\n",
    "            \n",
    "            preds = model(user_batch, movie_batch).squeeze()\n",
    "            loss = loss_fn(preds, y_batch.float())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_sq_error += torch.sum((preds - y_batch) ** 2).item()\n",
    "            total_samples += len(y_batch)\n",
    "    \n",
    "    test_loss = total_loss / len(test_loader)\n",
    "    test_rmse = (total_sq_error / total_samples) ** 0.5\n",
    "    test_losses.append(test_loss)\n",
    "    test_rmses.append(test_rmse)\n",
    "    \n",
    "    # ---------- Summary ----------\n",
    "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Test  Loss: {test_loss:.4f}, Test  RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bbf23-bc37-442c-826b-2c5754e80c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GDP]",
   "language": "python",
   "name": "conda-env-GDP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
